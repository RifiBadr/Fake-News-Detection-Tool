{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebaf39a8",
   "metadata": {},
   "source": [
    "# Fake News Detection Tool "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24547af",
   "metadata": {},
   "source": [
    "#Installing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08825252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bba68e",
   "metadata": {},
   "source": [
    "# Datasets (FakeNewsNet and LIAR)\n",
    "Datasets Download Link :https://ieee-dataport.org/open-access/fnid-fake-news-inference-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85779567",
   "metadata": {},
   "source": [
    "#Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692da1cc-cb96-462e-b063-a575d688c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_train = pd.read_csv('fnn_train.csv')\n",
    "fnn_dev = pd.read_csv('fnn_dev.csv')\n",
    "fnn_test = pd.read_csv('fnn_test.csv')\n",
    "\n",
    "liar_train = pd.read_csv('liar_train.csv')\n",
    "liar_dev = pd.read_csv('liar_dev.csv')\n",
    "liar_test = pd.read_csv('liar_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198e803",
   "metadata": {},
   "source": [
    "#Preprocessing and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6aff6a-257a-445c-88cf-c1b3560d7c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and preprocesses the input text with lemmatization.\"\"\"\n",
    "    # Lowercasing and removing unwanted patterns\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\", \" \", text) \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) \n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) \n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Stopword Removal and Lemmatization\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31ca70-70d5-4bf9-8dec-64d39fed8ae9",
   "metadata": {},
   "source": [
    "#Text Preprocessing Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090bac38-9d56-4b6e-87a4-e057c9344a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_and_measure(text):\n",
    "    \"\"\"Preprocess text and return both original and processed text lengths and word counts.\"\"\"\n",
    "    original_length = len(text)\n",
    "    original_word_count = len(text.split())\n",
    "    processed_text = preprocess_text(text)\n",
    "    processed_length = len(processed_text)\n",
    "    processed_word_count = len(processed_text.split())\n",
    "    return original_length, original_word_count, processed_length, processed_word_count\n",
    "\n",
    "def visualize_text_transformations(data, num_samples=100):\n",
    "    sampled_data = data.sample(n=num_samples, random_state=1)\n",
    "    measurements = sampled_data['statement'].apply(preprocess_text_and_measure)\n",
    "    measurements_df = pd.DataFrame(measurements.tolist(), columns=['Original Length', 'Original Words', 'Processed Length', 'Processed Words'])\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "    sns.barplot(data=measurements_df[['Original Length', 'Processed Length']], ax=axes[0], errorbar=None)\n",
    "    axes[0].set_title('Average Length of Statements')\n",
    "    axes[0].set_ylabel('Average Characters')\n",
    "    sns.barplot(data=measurements_df[['Original Words', 'Processed Words']], ax=axes[1], errorbar=None)\n",
    "    axes[1].set_title('Average Word Count of Statements')\n",
    "    axes[1].set_ylabel('Average Words')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    sns.histplot(measurements_df['Original Words'], bins=30, ax=axes[0], color='skyblue')\n",
    "    axes[0].set_title('Distribution of Word Counts (Original)')\n",
    "    sns.histplot(measurements_df['Processed Words'], bins=30, ax=axes[1], color='lightgreen')\n",
    "    axes[1].set_title('Distribution of Word Counts (Processed)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f0aa2-a584-46e3-9ffc-38557f60f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_text_transformations(pd.concat([fnn_train, liar_train]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d2da8-ea0b-4adf-bd20-ee3a4601037a",
   "metadata": {},
   "source": [
    "#Harmonizing Labels and Merging Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0a496-6e1c-4e72-b725-4b6ba505a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize_label(label):\n",
    "    if label in ['pants-fire', 'false', 'barely-true']:\n",
    "        return 'fake'\n",
    "    else:\n",
    "        return 'real'\n",
    "\n",
    "liar_train['label'] = liar_train['label-liar'].apply(harmonize_label)\n",
    "liar_dev['label'] = liar_dev['label-liar'].apply(harmonize_label)\n",
    "liar_test['label'] = liar_test['label-liar'].apply(harmonize_label)\n",
    "\n",
    "fnn_train.rename(columns={'label_fnn': 'label'}, inplace=True)\n",
    "fnn_dev.rename(columns={'label_fnn': 'label'}, inplace=True)\n",
    "fnn_test.rename(columns={'label_fnn': 'label'}, inplace=True)\n",
    "\n",
    "train = pd.concat([fnn_train[['statement', 'label']], liar_train[['statement', 'label']]])\n",
    "dev = pd.concat([fnn_dev[['statement', 'label']], liar_dev[['statement', 'label']]])\n",
    "test = pd.concat([fnn_test[['statement', 'label']], liar_test[['statement', 'label']]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82f43c7",
   "metadata": {},
   "source": [
    "#Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eee5fd-1cc0-4b6b-9be1-fe0596bc6f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train['statement'])\n",
    "X_test = vectorizer.transform(test['statement'])\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train['label'])\n",
    "y_test = label_encoder.transform(test['label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec13695-deb1-404c-ae1c-03b674552b26",
   "metadata": {},
   "source": [
    "#Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f31946-c978-41b0-b9f6-d7b386464dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    return model  \n",
    "# Logistic Regression\n",
    "print(\"Logistic Regression Results:\")\n",
    "lr = train_and_evaluate(LogisticRegression(), X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Decision Tree\n",
    "print(\"Decision Tree Results:\")\n",
    "dt = train_and_evaluate(DecisionTreeClassifier(), X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Random Forest\n",
    "print(\"Random Forest Results:\")\n",
    "rf = train_and_evaluate(RandomForestClassifier(), X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22b3a5-5e01-4082-9aba-584b883fe3d3",
   "metadata": {},
   "source": [
    "#Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f285aaa-8633-4b11-8827-9b33316fec7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models = ['Logistic Regression', 'Decision Tree', 'Random Forest']\n",
    "\n",
    "accuracy = [0.7426724137931034, 0.9482758620689655, 0.9439655172413793]\n",
    "\n",
    "\n",
    "precision_fake = [0.66, 0.91, 0.90]  # Class 0 (fake)\n",
    "precision_real = [0.83, 0.98, 0.98]  # Class 1 (real)\n",
    "\n",
    "recall_fake = [0.81, 0.97, 0.98]  # Class 0 (fake)\n",
    "recall_real = [0.70, 0.93, 0.92]  # Class 1 (real)\n",
    "\n",
    "f1_score_fake = [0.72, 0.94, 0.94]  # Class 0 (fake)\n",
    "f1_score_real = [0.76, 0.95, 0.95]  # Class 1 (real)\n",
    "\n",
    "def plot_metrics(metric_values, metric_name):\n",
    "    x = np.arange(len(models))  \n",
    "    width = 0.35  \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, np.array(metric_values[0]) * 100, width, label='Fake')\n",
    "    rects2 = ax.bar(x + width/2, np.array(metric_values[1]) * 100, width, label='Real')\n",
    "\n",
    "    ax.set_ylabel(f'{metric_name} (%)')\n",
    "    ax.set_title(f'{metric_name} by Model and Class (%)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for rect in rects1 + rects2:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.1f}%',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  \n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics([precision_fake, precision_real], 'Precision')\n",
    "plot_metrics([recall_fake, recall_real], 'Recall')\n",
    "plot_metrics([f1_score_fake, f1_score_real], 'F1 Score')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "rects = plt.bar(models, np.array(accuracy) * 100, color=['blue', 'orange', 'red'])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Model Accuracy Comparison (%)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for rect in rects:\n",
    "    height = rect.get_height()\n",
    "    plt.annotate(f'{height:.1f}%',\n",
    "                xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                xytext=(0, 3),  \n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29316b26-d9e7-4231-a24c-33ddf2621ee4",
   "metadata": {},
   "source": [
    "#Testing with Manual Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a994f-7be8-41ff-92c6-39f227b9174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_testing(news):\n",
    "    processed_news = preprocess_text(news)\n",
    "    vectorized_news = vectorizer.transform([processed_news])\n",
    "\n",
    "    lr_pred = lr.predict(vectorized_news)[0]\n",
    "    dt_pred = dt.predict(vectorized_news)[0]\n",
    "    rf_pred = rf.predict(vectorized_news)[0]\n",
    "\n",
    "    predictions = [lr_pred, dt_pred, rf_pred]\n",
    "    ensemble_pred = 1 if sum(predictions) > len(predictions) / 2 else 0\n",
    "    \n",
    "    print(\"Logistic Regression Prediction:\", label_encoder.inverse_transform([lr_pred])[0])\n",
    "    print(\"Decision Tree Prediction:\", label_encoder.inverse_transform([dt_pred])[0])\n",
    "    print(\"Random Forest Prediction:\", label_encoder.inverse_transform([rf_pred])[0])\n",
    "    print(\"Ensemble Prediction:\", label_encoder.inverse_transform([ensemble_pred])[0])\n",
    "\n",
    "news_input = input(\"Enter a piece of news to test: \")\n",
    "manual_testing(news_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ba137d-9885-47c3-bf2a-3a6d0c0d67a3",
   "metadata": {},
   "source": [
    "#Tool UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ce081-802b-467a-b8ab-d37dce0e9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<h1 style='text-align: center; color: black;'>Fake News Detection Tool</h1>\n",
    "\"\"\"))\n",
    "\n",
    "text_area = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type the news text here...',\n",
    "    description='',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%', height='150px', margin='0 auto')\n",
    ")\n",
    "submit_button = widgets.Button(\n",
    "    description='Check News',\n",
    "    button_style='success',  \n",
    "    tooltip='Click to check if the news is fake or real',\n",
    "    icon='check',\n",
    "    layout=widgets.Layout(width='30%', height='50px', margin='10px auto')\n",
    ")\n",
    "output = widgets.Output(layout={'border': '1px solid black', 'margin': '10px auto', 'width': '80%'})\n",
    "\n",
    "def on_submit_button_clicked(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        news_text = text_area.value.strip()\n",
    "        if not news_text:\n",
    "            print(\"Please enter the news text to check.\")\n",
    "            return\n",
    "        manual_testing(news_text)  \n",
    "submit_button.on_click(on_submit_button_clicked)\n",
    "display(widgets.VBox([text_area, submit_button, output], layout=widgets.Layout(align_items='center')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
